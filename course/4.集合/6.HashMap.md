# Map之HashMap

![Java-HashMap](https://tva1.sinaimg.cn/large/e6c9d24ely1go5v92ilnfj20xc0k00wo.jpg)

## [HashMap](https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html)

基于哈希表的Map接口的实现。此实现提供所有可选的映射操作，并允许空值和空键。 （HashMap类与Hashtable大致等效，不同之处在于它是不同步的，并且允许为null。）该类不保证映射的顺序；它不保证映射的顺序。特别是，它不能保证顺序会随着时间的推移保持恒定。

假设哈希函数将元素正确分散在存储桶中，则此实现为基本操作（获取和放置）提供恒定时间的性能。集合视图上的迭代所需的时间与HashMap实例的“容量”（存储桶数）及其大小（键-值映射数）成正比。因此，如果迭代性能很重要，则不要将初始容量设置得过高（或负载因子过低），这一点非常重要。

>An instance of HashMap has two parameters that affect its performance: initial capacity and load factor. The capacity is the number of buckets in the hash table, and the initial capacity is simply the capacity at the time the hash table is created. The load factor is a measure of how full the hash table is allowed to get before its capacity is automatically increased. When the number of entries in the hash table exceeds the product of the load factor and the current capacity, the hash table is rehashed (that is, internal data structures are rebuilt) so that the hash table has approximately twice the number of buckets.

>As a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the HashMap class, including get and put). The expected number of entries in the map and its load factor should be taken into account when setting its initial capacity, so as to minimize the number of rehash operations. If the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.

通常，默认负载因子（0.75）在时间和空间成本之间提供了一个很好的折衷方案。较高的值会减少空间开销，但会增加查找成本（在HashMap类的大多数操作中都得到体现，包括get和put）。设置其初始容量时，应考虑映射中的预期条目数及其负载因子，以最大程度地减少重新哈希操作的数量。如果初始容量大于最大条目数除以负载因子，则将不会进行任何哈希操作。

>If many mappings are to be stored in a HashMap instance, creating it with a sufficiently large capacity will allow the mappings to be stored more efficiently than letting it perform automatic rehashing as needed to grow the table. Note that using many keys with the same hashCode() is a sure way to slow down performance of any hash table. To ameliorate impact, when keys are Comparable, this class may use comparison order among keys to help break ties.

如果许多映射要存储在一个HashMap实例中，那么使用足够大的容量创建它将使映射能够更有效地存储，而不是根据需要执行自动重新散列来增长表。注意，使用多个键和相同的hashCode()肯定会降低任何哈希表的性能。为了减轻影响，当键具有可比性时，该类可以使用键之间的比较顺序来帮助打破联系。

**Note that this implementation is not synchronized.** If multiple threads access a hash map concurrently, and at least one of the threads modifies the map structurally, it must be synchronized externally. (A structural modification is any operation that adds or deletes one or more mappings; merely changing the value associated with a key that an instance already contains is not a structural modification.) This is typically accomplished by synchronizing on some object that naturally encapsulates the map. If no such object exists, the map should be "wrapped" using the Collections.synchronizedMap method. This is best done at creation time, to prevent accidental unsynchronized access to the map:`Map m = Collections.synchronizedMap(new HashMap(...));`

## 创建与遍历

### 构造函数

HashMap为我们提供了四种构造函数：

* HashMap()：Constructs an empty HashMap with the default initial capacity (16) and the default load factor (0.75).
* HashMap(int initialCapacity)：Constructs an empty HashMap with the specified initial capacity and the default load factor (0.75).
* HashMap(int initialCapacity, float loadFactor)：Constructs an empty HashMap with the specified initial capacity and load factor.
* HashMap(Map<? extends K,? extends V> m)：Constructs a new HashMap with the same mappings as the specified Map.

### 遍历

1. **通过KeySet集合来遍历**

```java
Map<Integer, String> map = new HashMap<>();
for (Integer key : map.keySet()) {
    System.out.println(key);
    System.out.println(map.get(key));
}
```

2. **通过Entry接口来遍历**

```java
Map<Integer, String> map = new HashMap<>();
for(Map.Entry<Integer, String> entry : map.entrySet()) {
    System.out.println(entry.getKey());
    System.out.println(entry.getValue());
}
```

3. **通过迭代器遍历**

```java
Map<Integer, String> map = new HashMap<>();
Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();
while (iterator.hasNext()) {
    Map.Entry<Integer, String> entry = iterator.next();
    System.out.println(entry.getKey());
    System.out.println(entry.getValue());
}
```

4. **通过Java8的forEach方法来遍历**

```java
Map<Integer, String> map = new HashMap<>();
map.forEach((key, value) -> {
    System.out.println(key);
    System.out.println(value);
});
```

### 四种遍历方法的性能对比

我们使用10000000条数据用于测试，测试代码如下：

```java
public class Main {
    public static void main(String[] args) throws InterruptedException {
        Map<Integer, String> map = new HashMap<>();
        for (int i = 0; i < 10000000; i++) {
            map.put(i, "value" + i);
        }
        keySet(map);
        entry(map);
        iterator(map);
        foreach(map);
    }

    private static void keySet(Map<Integer, String> map) {
        Instant startTime = Instant.now();
        for (Integer key : map.keySet()) {
            // test
        }
        System.out.println("keySet Speed Time: " + (Instant.now().getNano() - startTime.getNano()));
    }

    private static void entry(Map<Integer, String> map) {
        Instant startTime = Instant.now();
        for (Map.Entry<Integer, String> entry : map.entrySet()) {
            // test
        }
        System.out.println("Entry Speed Time: " + (Instant.now().getNano() - startTime.getNano()));
    }

    private static void iterator(Map<Integer, String> map) {
        Instant startTime = Instant.now();
        Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();
        while (iterator.hasNext()) {
            Map.Entry<Integer, String> entry = iterator.next();
        }
        System.out.println("Iterator Speed Time: " + (Instant.now().getNano() - startTime.getNano()));
    }

    private static void foreach(Map<Integer, String> map) {
        Instant startTime = Instant.now();
        map.forEach((key, value) -> {
            //test
        });
        System.out.println("Foreach Speed Time: " + (Instant.now().getNano() - startTime.getNano()));
    }
```

输出结果为：
```bash
keySet Speed Time: 155159000
Entry Speed Time: 118331000
Iterator Speed Time: 128656000
Foreach Speed Time: 96296000
```

多次运行，结果与上面的类似，这就说明，当数据量很大的情况下，`forEach`遍历最快，`keySet`遍历最慢，其他两种旗鼓相当。

## 底层原理与实现

### 构造函数的底层原理

从上面的构造函数来看，无参的构造函数初始容量为16，负载因子是0.75，这就意味着，使用无参的构造函数创建的Map集合中存储的对象数量大于12(16 * 0.75 = 12)个时，该集合将扩容至32(一般扩容的标准是当前容量的两倍)，扩容就意味着原来集合的元素将迁移到新的集合中，重新计算它的位置，这个过程称之为`ReHash`, 当新Map集合的元素数量超过24(32 * 0.75 = 24)时, 将再次扩容，后面的情况以此类推。

对于有参数的构造函数来说，我们可以在初始化Map集合时指定其容量，如果我们创建`new HashMap<>(10)`，是否意味着集合的容量为10？其实并不是，HashMap的构造函数对参数进行的优化，容量大于参数指定容量的最小的2的n次方，也就是16(2的四次方)。之所以会这么定义，一方面是因为对于默认的负载因子来说，2的n次方乘以0.75将是一个正整数；另一方面，Java工程师经过多次测试，这个容量对于实际情况而言效果最佳。所以，这个构造函数给我们提供了一种根据实际需要来指定容量的方法，避免了HashMap频繁扩容而产生的性能消耗。

当然，另一个构造函数可以传入初始容量和负载因子，但是通常情况下，我们不建议修改负载因子，默认负载因子（0.75）在时间和空间成本之间提供了一个很好的折衷方案。较高的值会减少空间开销，但会增加查找成本，这是Java工程师经过多次调试才计算出的一个最佳值。

### Hash存储原理

HashMap里面使用`transient Node<K,V>[] table`用于存储元素，这是一个Node的数组，Node的结构如下：

```java
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    V value;
    Node<K,V> next;

    Node(int hash, K key, V value, Node<K,V> next) {
        this.hash = hash;
        this.key = key;
        this.value = value;
        this.next = next;
    }

    public final K getKey()        { return key; }
    public final V getValue()      { return value; }
    public final String toString() { return key + "=" + value; }

    public final int hashCode() {
        return Objects.hashCode(key) ^ Objects.hashCode(value);
    }

    public final V setValue(V newValue) {
        V oldValue = value;
        value = newValue;
        return oldValue;
    }

    public final boolean equals(Object o) {
        if (o == this)
            return true;
        if (o instanceof Map.Entry) {
            Map.Entry<?,?> e = (Map.Entry<?,?>)o;
            if (Objects.equals(key, e.getKey()) &&
                Objects.equals(value, e.getValue()))
                return true;
        }
        return false;
    }
}
```

这个Node包括两个方面的信息，一个是自身元素，另一个是指向下一个元素的引用。同时该类最重要的方法是`hashCode()`,这个方法对key和value分别取Hash值，然后通过`与运算`获得一个正整数。在HashMap中，这个正整数将用于元素位置(数组的下标)的计算。

#### 存储结构
从结构实现来讲，HashMap是Node数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。

![hashmap-structure](https://tva1.sinaimg.cn/large/e6c9d24ely1gokz9zz8m9j20qy0ke0t6.jpg)

从上面的结构图可以看出，Node数组用来存储元素，一个元素会经过一系列的算法来确定其下标，如果两个元素的下标相同，将使用链表来存储，Node元素中next指向的就是下一个元素。当链表的长度超过8，链表的结构将变成红黑树的结构存储。

所以，最关键的部分是，如何确定存储的元素在Node数组中的下标位置？

#### 确定元素在Node数组中的下标位置

准确来说，获取数组的下标总共分为三步：

1. **获取元素本身的HashCode**

元素的key和value将被转换为Node对象存储在Node数组中，Node类提供了一个`hashCode()`方法来获取其HashCode，这个方法对key和value分别取Hash值：

```java
public final int hashCode() {
    return Objects.hashCode(key) ^ Objects.hashCode(value);
}
```

2. **通过高位运算处理Node元素的HashCode**

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h >>> 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销
下面举例说明下，n为table的长度。

![hashmap-gaowei](https://tva1.sinaimg.cn/large/e6c9d24ely1gokzu34f8xj20xa0j0jt9.jpg)

3. **对Hash值与数组长度进行取模运算**

```java
static int indexFor(int h, int length) {  //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的
     return h & (length-1);  //第三步 取模运算
}
```

这个方法非常巧妙，它通过h & (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h& (length-1)运算等价于对length取模，也就是h%length，但是&比%具有更高的效率。

#### put(K, V)方法执行过程

下面是`put()`方法源码：

```java
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    //如果当前HashMap的table数组还未定义或者还未初始化其长度，则先通过resize()进行扩容,返回扩容后的数组长度n
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    //通过数组长度与hash值做按位与&运算得到对应数组下标，若该位置没有元素，则new Node直接将新元素插入
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    //否则该位置已经存在元素，进入else里面执行
    else {
        Node<K,V> e; K k;
        //key的hash和key本身都相同，则用e保存p
        if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        //key的hash相同，key不同，如果是红黑树，插入到红黑树上
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        //如果不是TreeNode,则进行链表遍历
        else {
            for (int binCount = 0; ; ++binCount) {
                 /**
                 * 在链表最后一个节点之后并没有找到相同的元素，则进行下面的操作，直接new Node插入，
                 * 但条件判断有可能转化为红黑树
                 */
                if ((e = p.next) == null) {
                    //直接new了一个Node
                    p.next = newNode(hash, key, value, null);
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                /**
                 * 如果在链表的最后一个节点之前找到key值相同的（和上面的判断不冲突，上面是直接通过数组
                 * 下标判断key值是否相同），则替换
                 */
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
             //onlyIfAbsent为true时:当某个位置已经存在元素时不去覆盖
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
```

执行流程图如下：

![hashmap-put](https://tva1.sinaimg.cn/large/e6c9d24ely1gol011uhlpj211v0u0gqk.jpg)














